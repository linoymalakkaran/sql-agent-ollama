{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e631d4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain langchain-openai langchain-huggingface langchain-community python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3c258e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint, HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e26ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ RUN ALL PREREQUISITES - Execute this first!\n",
    "print(\"üîß Setting up all prerequisites...\")\n",
    "\n",
    "# 1. Import required libraries (already done in cell 2)\n",
    "print(\"‚úÖ 1. Libraries imported\")\n",
    "\n",
    "# 2. Setup database connection with auto-download\n",
    "import os\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the current notebook directory (where the .ipynb file is located)\n",
    "notebook_dir = Path.cwd()\n",
    "db_path = notebook_dir / \"Chinook.db\"\n",
    "\n",
    "print(f\"üîç 2. Looking for database at: {db_path}\")\n",
    "print(f\"   Database exists: {db_path.exists()}\")\n",
    "\n",
    "# Download Chinook database if it doesn't exist\n",
    "if not db_path.exists():\n",
    "    print(\"üì• Chinook.db not found. Downloading...\")\n",
    "    try:\n",
    "        # Download Chinook database from GitHub\n",
    "        chinook_url = \"https://github.com/lerocha/chinook-database/raw/master/ChinookDatabase/DataSources/Chinook_Sqlite.sqlite\"\n",
    "        \n",
    "        print(\"   ‚è≥ Downloading Chinook database...\")\n",
    "        urllib.request.urlretrieve(chinook_url, str(db_path))\n",
    "        print(f\"   ‚úÖ Downloaded Chinook.db ({db_path.stat().st_size} bytes)\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Download failed: {e}\")\n",
    "        print(\"   üí° Please manually download Chinook.db from:\")\n",
    "        print(\"   https://github.com/lerocha/chinook-database/raw/master/ChinookDatabase/DataSources/Chinook_Sqlite.sqlite\")\n",
    "        raise\n",
    "else:\n",
    "    print(f\"   Database size: {db_path.stat().st_size} bytes\")\n",
    "\n",
    "# Create database connection with relative path\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "db = SQLDatabase.from_uri(f\"sqlite:///{db_path}\", sample_rows_in_table_info=0)\n",
    "\n",
    "def get_schema(_):\n",
    "    return db.get_table_info()\n",
    "\n",
    "def run_query(query):\n",
    "    print(f'Query being run: {query} \\n\\n')\n",
    "    return db.run(query)\n",
    "\n",
    "print(\"‚úÖ 2. Database connection established\")\n",
    "\n",
    "# 3. Load environment variables with relative path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Look for .env file in the same directory as the notebook\n",
    "env_path = notebook_dir / \".env\"\n",
    "print(f\"üîç 3. Looking for .env file at: {env_path}\")\n",
    "\n",
    "if env_path.exists():\n",
    "    # Load with explicit path\n",
    "    load_result = load_dotenv(dotenv_path=str(env_path), override=True)\n",
    "    print(f\"   ‚úÖ .env file loaded: {load_result}\")\n",
    "else:\n",
    "    # Try loading from current directory\n",
    "    load_result = load_dotenv(override=True)\n",
    "    print(f\"   ‚ö†Ô∏è  .env file not found at {env_path}\")\n",
    "    print(f\"   üí° Create a .env file in the same directory as your notebook\")\n",
    "\n",
    "# Check API tokens\n",
    "hf_token = os.getenv('HUGGINGFACEHUB_API_TOKEN')\n",
    "openai_token = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "print(\"üîë API Token Status:\")\n",
    "if hf_token:\n",
    "    print(f\"   ‚úÖ HuggingFace token: {hf_token[:10]}...\")\n",
    "else:\n",
    "    print(\"   ‚ùå HuggingFace token: Not found\")\n",
    "    \n",
    "if openai_token:\n",
    "    print(f\"   ‚úÖ OpenAI token: {openai_token[:10]}...\")\n",
    "else:\n",
    "    print(\"   ‚ùå OpenAI token: Not found\")\n",
    "\n",
    "# Display file structure\n",
    "print(f\"\\nüìÅ Current working directory: {notebook_dir}\")\n",
    "print(\"üìã Project files:\")\n",
    "important_files = [\".env\", \"Chinook.db\", \"agent.ipynb\", \".gitignore\", \"README.md\"]\n",
    "for file in important_files:\n",
    "    file_path = notebook_dir / file\n",
    "    if file_path.exists():\n",
    "        if file.endswith('.db'):\n",
    "            size = f\" ({file_path.stat().st_size:,} bytes)\"\n",
    "        else:\n",
    "            size = \"\"\n",
    "        print(f\"   ‚úÖ {file}{size}\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå {file} (missing)\")\n",
    "\n",
    "print(\"\\nüéØ Prerequisites complete! Now you can run the other cells.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7602cf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ü§ñ UNIFIED LLM CREATION - Ollama, HuggingFace & OpenAI\n",
    "\n",
    "def create_llm(provider=\"ollama\", model_name=None):\n",
    "    \"\"\"\n",
    "    Create LLM with automatic fallback priority: Ollama -> HuggingFace -> OpenAI\n",
    "    \n",
    "    Args:\n",
    "        provider: \"ollama\", \"huggingface\", or \"openai\" \n",
    "        model_name: Specific model name (optional)\n",
    "    \n",
    "    Returns:\n",
    "        LLM instance or None if failed\n",
    "    \"\"\"\n",
    "    \n",
    "    # Try Ollama first (best for local use)\n",
    "    if provider == \"ollama\" or provider == \"auto\":\n",
    "        try:\n",
    "            from langchain_ollama import ChatOllama\n",
    "            import requests\n",
    "            \n",
    "            # Check if Ollama is running\n",
    "            response = requests.get(\"http://localhost:11434/api/tags\", timeout=3)\n",
    "            if response.status_code == 200:\n",
    "                models_data = response.json()\n",
    "                available_models = [m['name'] for m in models_data.get('models', []) if 'embed' not in m['name'].lower()]\n",
    "                \n",
    "                if available_models:\n",
    "                    # Use specified model or best available\n",
    "                    if model_name and model_name in available_models:\n",
    "                        selected_model = model_name\n",
    "                    else:\n",
    "                        # Priority: llama3.2:3b > llama3.2:1b > llama3.2 > first available\n",
    "                        preferred = ['llama3.2:3b', 'llama3.2:1b', 'llama3.2']\n",
    "                        selected_model = next((m for m in preferred if m in available_models), available_models[0])\n",
    "                    \n",
    "                    ollama_llm = ChatOllama(\n",
    "                        model=selected_model,\n",
    "                        base_url=\"http://localhost:11434\",\n",
    "                        temperature=0.1\n",
    "                    )\n",
    "                    \n",
    "                    print(f\"‚úÖ Created Ollama LLM: {selected_model}\")\n",
    "                    return ollama_llm\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Ollama failed: {e}\")\n",
    "    \n",
    "    # Try HuggingFace second\n",
    "    if provider == \"huggingface\" or provider == \"auto\":\n",
    "        try:\n",
    "            from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "            import os\n",
    "            \n",
    "            hf_token = os.getenv('HUGGINGFACEHUB_API_TOKEN')\n",
    "            if hf_token and hf_token.startswith('hf_'):\n",
    "                hf_model = model_name or \"microsoft/DialoGPT-medium\"\n",
    "                \n",
    "                llm = HuggingFaceEndpoint(\n",
    "                    repo_id=hf_model,\n",
    "                    task=\"text-generation\",\n",
    "                    temperature=0.1,\n",
    "                    max_new_tokens=512\n",
    "                )\n",
    "                hf_llm = ChatHuggingFace(llm=llm)\n",
    "                \n",
    "                print(f\"‚úÖ Created HuggingFace LLM: {hf_model}\")\n",
    "                return hf_llm\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  HuggingFace failed: {e}\")\n",
    "    \n",
    "    # Try OpenAI last\n",
    "    if provider == \"openai\" or provider == \"auto\":\n",
    "        try:\n",
    "            from langchain_openai import ChatOpenAI\n",
    "            import os\n",
    "            \n",
    "            openai_token = os.getenv('OPENAI_API_KEY')\n",
    "            if openai_token and openai_token != \"your_openai_api_key_here\":\n",
    "                openai_model = model_name or \"gpt-4\"\n",
    "                \n",
    "                openai_llm = ChatOpenAI(\n",
    "                    model=openai_model,\n",
    "                    temperature=0.1\n",
    "                )\n",
    "                \n",
    "                print(f\"‚úÖ Created OpenAI LLM: {openai_model}\")\n",
    "                return openai_llm\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  OpenAI failed: {e}\")\n",
    "    \n",
    "    print(\"‚ùå All LLM providers failed\")\n",
    "    return None\n",
    "\n",
    "# SQL Agent Functions\n",
    "def write_sql_query(llm):\n",
    "    \"\"\"Generate SQL query from natural language\"\"\"\n",
    "    template = \"\"\"Based on the table schema below, write a SQL query that would answer the user's question:\n",
    "    {schema}\n",
    "\n",
    "    Question: {question}\n",
    "    SQL Query:\"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Given an input question, convert it to a SQL query. No pre-amble. Return only the SQL query.\"),\n",
    "        (\"human\", template),\n",
    "    ])\n",
    "\n",
    "    return (\n",
    "        RunnablePassthrough.assign(schema=get_schema)\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "def answer_user_query(query, llm):\n",
    "    \"\"\"Complete SQL agent - generates SQL, executes it, and provides natural language response\"\"\"\n",
    "    template = \"\"\"Based on the table schema below, question, sql query, and sql response, write a natural language response:\n",
    "    {schema}\n",
    "\n",
    "    Question: {question}\n",
    "    SQL Query: {query}\n",
    "    SQL Response: {response}\"\"\"\n",
    "\n",
    "    prompt_response = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Given an input question and SQL response, convert it to a natural language answer. No pre-amble.\"),\n",
    "        (\"human\", template),\n",
    "    ])\n",
    "\n",
    "    full_chain = (\n",
    "        RunnablePassthrough.assign(query=write_sql_query(llm))\n",
    "        | RunnablePassthrough.assign(\n",
    "            schema=get_schema,\n",
    "            response=lambda x: run_query(x[\"query\"]),\n",
    "        )\n",
    "        | prompt_response\n",
    "        | llm\n",
    "    )\n",
    "\n",
    "    return full_chain.invoke({\"question\": query})\n",
    "\n",
    "print(\"‚úÖ Unified LLM functions created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a822b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_user_query(query, llm):\n",
    "    template = \"\"\"Based on the table schema below, question, sql query, and sql response, write a natural language response:\n",
    "    {schema}\n",
    "\n",
    "    Question: {question}\n",
    "    SQL Query: {query}\n",
    "    SQL Response: {response}\"\"\"\n",
    "\n",
    "    prompt_response = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"Given an input question and SQL response, convert it to a natural language answer. No pre-amble.\",\n",
    "            ),\n",
    "            (\"human\", template),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    full_chain = (\n",
    "        RunnablePassthrough.assign(query=write_sql_query(llm))\n",
    "        | RunnablePassthrough.assign(\n",
    "            schema=get_schema,\n",
    "            response=lambda x: run_query(x[\"query\"]),\n",
    "        )\n",
    "        | prompt_response\n",
    "        | llm\n",
    "    )\n",
    "\n",
    "    return full_chain.invoke({\"question\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4b6f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üõ†Ô∏è HELPER FUNCTIONS\n",
    "\n",
    "def simple_db_query(sql_query):\n",
    "    \"\"\"Run a direct SQL query on the database\"\"\"\n",
    "    try:\n",
    "        return db.run(sql_query)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def ask_database(question):\n",
    "    \"\"\"Simple function to ask questions to your database\"\"\"\n",
    "    if 'working_llm' in globals():\n",
    "        try:\n",
    "            response = answer_user_query(question, llm=working_llm)\n",
    "            return response.content\n",
    "        except Exception as e:\n",
    "            return f\"Error: {e}\"\n",
    "    else:\n",
    "        return \"Error: No LLM available. Run the setup cells first.\"\n",
    "\n",
    "def show_database_info():\n",
    "    \"\"\"Display basic information about the database\"\"\"\n",
    "    try:\n",
    "        artists = simple_db_query(\"SELECT COUNT(*) FROM Artist\")[0][0]\n",
    "        albums = simple_db_query(\"SELECT COUNT(*) FROM Album\")[0][0] \n",
    "        tracks = simple_db_query(\"SELECT COUNT(*) FROM Track\")[0][0]\n",
    "        genres = simple_db_query(\"SELECT COUNT(*) FROM Genre\")[0][0]\n",
    "        \n",
    "        print(\"üìä Database Information:\")\n",
    "        print(f\"   Artists: {artists}\")\n",
    "        print(f\"   Albums: {albums}\")\n",
    "        print(f\"   Tracks: {tracks}\")\n",
    "        print(f\"   Genres: {genres}\")\n",
    "        \n",
    "        print(f\"\\nüéµ Sample Artists:\")\n",
    "        sample_artists = simple_db_query(\"SELECT Name FROM Artist LIMIT 5\")\n",
    "        for i, (name,) in enumerate(sample_artists, 1):\n",
    "            print(f\"   {i}. {name}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting database info: {e}\")\n",
    "\n",
    "def check_project_setup():\n",
    "    \"\"\"Check if all project files are properly set up\"\"\"\n",
    "    from pathlib import Path\n",
    "    \n",
    "    current_dir = Path.cwd()\n",
    "    print(f\"üìÅ Project Directory: {current_dir}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    files_to_check = {\n",
    "        \"Chinook.db\": \"SQLite database file\",\n",
    "        \".env\": \"Environment variables (API keys)\",\n",
    "        \"agent.ipynb\": \"Main notebook file\",\n",
    "        \".gitignore\": \"Git ignore rules\",\n",
    "        \"README.md\": \"Project documentation\"\n",
    "    }\n",
    "    \n",
    "    all_good = True\n",
    "    for filename, description in files_to_check.items():\n",
    "        file_path = current_dir / filename\n",
    "        if file_path.exists():\n",
    "            if filename.endswith('.db'):\n",
    "                size_mb = file_path.stat().st_size / (1024 * 1024)\n",
    "                print(f\"‚úÖ {filename} - {description} ({size_mb:.1f} MB)\")\n",
    "            else:\n",
    "                print(f\"‚úÖ {filename} - {description}\")\n",
    "        else:\n",
    "            print(f\"‚ùå {filename} - {description} (MISSING)\")\n",
    "            all_good = False\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    if all_good:\n",
    "        print(\"üéâ All project files are present!\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Some files are missing. Run the setup cells to fix.\")\n",
    "    \n",
    "    return all_good\n",
    "\n",
    "def download_chinook_if_missing():\n",
    "    \"\"\"Download Chinook database if it's missing\"\"\"\n",
    "    from pathlib import Path\n",
    "    import urllib.request\n",
    "    \n",
    "    db_path = Path.cwd() / \"Chinook.db\"\n",
    "    \n",
    "    if not db_path.exists():\n",
    "        print(\"üì• Downloading Chinook database...\")\n",
    "        try:\n",
    "            chinook_url = \"https://github.com/lerocha/chinook-database/raw/master/ChinookDatabase/DataSources/Chinook_Sqlite.sqlite\"\n",
    "            urllib.request.urlretrieve(chinook_url, str(db_path))\n",
    "            print(f\"‚úÖ Downloaded Chinook.db ({db_path.stat().st_size:,} bytes)\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Download failed: {e}\")\n",
    "            return False\n",
    "    else:\n",
    "        print(f\"‚úÖ Chinook.db already exists ({db_path.stat().st_size:,} bytes)\")\n",
    "        return True\n",
    "\n",
    "print(\"‚úÖ Helper functions created:\")\n",
    "print(\"   ‚Ä¢ simple_db_query('SQL HERE')\")\n",
    "print(\"   ‚Ä¢ ask_database('Your question')\")\n",
    "print(\"   ‚Ä¢ show_database_info()\")\n",
    "print(\"   ‚Ä¢ check_project_setup()\")\n",
    "print(\"   ‚Ä¢ download_chinook_if_missing()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43b19cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß PROJECT SETUP VERIFICATION\n",
    "\n",
    "print(\"üîç Verifying project setup...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check all project files\n",
    "setup_ok = check_project_setup()\n",
    "\n",
    "# Show database info if available\n",
    "if setup_ok:\n",
    "    print(f\"\\nüìä Database quick check:\")\n",
    "    try:\n",
    "        show_database_info()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Database error: {e}\")\n",
    "        print(\"üí° Try running the prerequisites cell again\")\n",
    "\n",
    "print(f\"\\nüí° Quick Commands:\")\n",
    "print(\"‚Ä¢ check_project_setup() - Verify all files\")\n",
    "print(\"‚Ä¢ download_chinook_if_missing() - Download database if missing\")\n",
    "print(\"‚Ä¢ show_database_info() - Show database statistics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e2e323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ SQL AGENT - Simple Demo\n",
    "\n",
    "print(\"\udd16 Creating SQL Agent...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create LLM (tries Ollama first, then HuggingFace, then OpenAI)\n",
    "working_llm = create_llm(\"auto\")\n",
    "\n",
    "if working_llm is None:\n",
    "    print(\"‚ùå No LLM available. Make sure you have:\")\n",
    "    print(\"   ‚Ä¢ Ollama running with a model (ollama pull llama3.2)\")\n",
    "    print(\"   ‚Ä¢ OR HuggingFace token in .env file\")\n",
    "    print(\"   ‚Ä¢ OR OpenAI API key in .env file\")\n",
    "else:\n",
    "    print(f\"‚úÖ LLM ready: {type(working_llm).__name__}\")\n",
    "    \n",
    "    # Test the SQL agent with sample questions\n",
    "    test_questions = [\n",
    "        \"How many artists are in the database?\",\n",
    "        \"Show me 3 artists whose names start with 'A'\",\n",
    "        \"How many albums are there?\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nüß™ Testing SQL Agent...\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    for i, question in enumerate(test_questions, 1):\n",
    "        print(f\"\\n{i}. Question: {question}\")\n",
    "        try:\n",
    "            response = answer_user_query(question, llm=working_llm)\n",
    "            print(f\"   Answer: {response.content}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   Error: {e}\")\n",
    "            # Fallback to direct database query\n",
    "            if \"artists\" in question.lower():\n",
    "                fallback = simple_db_query(\"SELECT COUNT(*) FROM Artist\")\n",
    "                print(f\"   Fallback: {fallback[0][0]} artists found\")\n",
    "            elif \"albums\" in question.lower():\n",
    "                fallback = simple_db_query(\"SELECT COUNT(*) FROM Album\") \n",
    "                print(f\"   Fallback: {fallback[0][0]} albums found\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 50)\n",
    "    print(\"üéâ SQL Agent Ready! Use:\")\n",
    "    print(\"answer_user_query('Your question here', llm=working_llm)\")\n",
    "    print(\"simple_db_query('SELECT * FROM Artist LIMIT 5')\")\n",
    "    \n",
    "    # Save for easy access\n",
    "    globals()['sql_agent_llm'] = working_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75435b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \udca1 USAGE EXAMPLES\n",
    "\n",
    "print(\"üí° How to use your SQL Agent:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Example 1: Ask database questions\n",
    "print(\"\\n1Ô∏è‚É£ Ask natural language questions:\")\n",
    "print(\"answer_user_query('How many tracks are there?', llm=working_llm)\")\n",
    "print(\"answer_user_query('What genres are available?', llm=working_llm)\")\n",
    "print(\"answer_user_query('Which artist has the most albums?', llm=working_llm)\")\n",
    "\n",
    "# Example 2: Direct SQL queries  \n",
    "print(\"\\n2Ô∏è‚É£ Run direct SQL queries:\")\n",
    "print(\"simple_db_query('SELECT * FROM Artist LIMIT 5')\")\n",
    "print(\"simple_db_query('SELECT COUNT(*) FROM Track')\")\n",
    "print(\"simple_db_query('SELECT DISTINCT Name FROM Genre')\")\n",
    "\n",
    "# Example 3: Direct LLM chat\n",
    "print(\"\\n3Ô∏è‚É£ Chat directly with LLM:\")\n",
    "print(\"working_llm.invoke([HumanMessage(content='Explain what SQL is')])\")\n",
    "\n",
    "print(\"\\nüéØ Try one now:\")\n",
    "if 'working_llm' in globals():\n",
    "    try:\n",
    "        # Quick test\n",
    "        result = answer_user_query(\"How many artists are there?\", llm=working_llm)\n",
    "        print(f\"‚úÖ Test result: {result.content}\")\n",
    "    except:\n",
    "        result = simple_db_query(\"SELECT COUNT(*) FROM Artist\")\n",
    "        print(f\"‚úÖ Database has {result[0][0]} artists\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Run the previous cell first to create working_llm\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
