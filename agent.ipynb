{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e631d4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain langchain-openai langchain-huggingface langchain-community python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3c258e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint, HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e26ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ RUN ALL PREREQUISITES - Execute this first!\n",
    "print(\"üîß Setting up all prerequisites...\")\n",
    "\n",
    "# 1. Import required libraries (already done in cell 2)\n",
    "print(\"‚úÖ 1. Libraries imported\")\n",
    "\n",
    "# 2. Setup database connection\n",
    "import os\n",
    "notebook_dir = r\"c:\\Users\\malakkaran.pappachan\\OneDrive - Abu Dhabi Ports\\Desktop\\test\"\n",
    "db_path = os.path.join(notebook_dir, \"Chinook.db\")\n",
    "\n",
    "print(f\"üîç 2. Looking for database at: {db_path}\")\n",
    "print(f\"   Database exists: {os.path.exists(db_path)}\")\n",
    "if os.path.exists(db_path):\n",
    "    print(f\"   Database size: {os.path.getsize(db_path)} bytes\")\n",
    "\n",
    "# Create database connection with the correct path\n",
    "db = SQLDatabase.from_uri(f\"sqlite:///{db_path}\", sample_rows_in_table_info=0)\n",
    "\n",
    "def get_schema(_):\n",
    "    return db.get_table_info()\n",
    "\n",
    "def run_query(query):\n",
    "    print(f'Query being run: {query} \\n\\n')\n",
    "    return db.run(query)\n",
    "\n",
    "print(\"‚úÖ 2. Database connection established\")\n",
    "\n",
    "# 3. Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Check API tokens\n",
    "hf_token = os.getenv('HUGGINGFACEHUB_API_TOKEN')\n",
    "openai_token = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "print(\"üîë 3. API Token Status:\")\n",
    "if hf_token:\n",
    "    print(f\"   ‚úÖ HuggingFace token: {hf_token[:10]}...\")\n",
    "else:\n",
    "    print(\"   ‚ùå HuggingFace token: Not found\")\n",
    "    \n",
    "if openai_token:\n",
    "    print(f\"   ‚úÖ OpenAI token: {openai_token[:10]}...\")\n",
    "else:\n",
    "    print(\"   ‚ùå OpenAI token: Not found\")\n",
    "\n",
    "print(\"\\nüéØ Prerequisites complete! Now you can run the other cells.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7602cf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ü§ñ UNIFIED LLM CREATION - Ollama, HuggingFace & OpenAI\n",
    "\n",
    "def create_llm(provider=\"ollama\", model_name=None):\n",
    "    \"\"\"\n",
    "    Create LLM with automatic fallback priority: Ollama -> HuggingFace -> OpenAI\n",
    "    \n",
    "    Args:\n",
    "        provider: \"ollama\", \"huggingface\", or \"openai\" \n",
    "        model_name: Specific model name (optional)\n",
    "    \n",
    "    Returns:\n",
    "        LLM instance or None if failed\n",
    "    \"\"\"\n",
    "    \n",
    "    # Try Ollama first (best for local use)\n",
    "    if provider == \"ollama\" or provider == \"auto\":\n",
    "        try:\n",
    "            from langchain_ollama import ChatOllama\n",
    "            import requests\n",
    "            \n",
    "            # Check if Ollama is running\n",
    "            response = requests.get(\"http://localhost:11434/api/tags\", timeout=3)\n",
    "            if response.status_code == 200:\n",
    "                models_data = response.json()\n",
    "                available_models = [m['name'] for m in models_data.get('models', []) if 'embed' not in m['name'].lower()]\n",
    "                \n",
    "                if available_models:\n",
    "                    # Use specified model or best available\n",
    "                    if model_name and model_name in available_models:\n",
    "                        selected_model = model_name\n",
    "                    else:\n",
    "                        # Priority: llama3.2:3b > llama3.2:1b > llama3.2 > first available\n",
    "                        preferred = ['llama3.2:3b', 'llama3.2:1b', 'llama3.2']\n",
    "                        selected_model = next((m for m in preferred if m in available_models), available_models[0])\n",
    "                    \n",
    "                    ollama_llm = ChatOllama(\n",
    "                        model=selected_model,\n",
    "                        base_url=\"http://localhost:11434\",\n",
    "                        temperature=0.1\n",
    "                    )\n",
    "                    \n",
    "                    print(f\"‚úÖ Created Ollama LLM: {selected_model}\")\n",
    "                    return ollama_llm\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Ollama failed: {e}\")\n",
    "    \n",
    "    # Try HuggingFace second\n",
    "    if provider == \"huggingface\" or provider == \"auto\":\n",
    "        try:\n",
    "            from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "            import os\n",
    "            \n",
    "            hf_token = os.getenv('HUGGINGFACEHUB_API_TOKEN')\n",
    "            if hf_token and hf_token.startswith('hf_'):\n",
    "                hf_model = model_name or \"microsoft/DialoGPT-medium\"\n",
    "                \n",
    "                llm = HuggingFaceEndpoint(\n",
    "                    repo_id=hf_model,\n",
    "                    task=\"text-generation\",\n",
    "                    temperature=0.1,\n",
    "                    max_new_tokens=512\n",
    "                )\n",
    "                hf_llm = ChatHuggingFace(llm=llm)\n",
    "                \n",
    "                print(f\"‚úÖ Created HuggingFace LLM: {hf_model}\")\n",
    "                return hf_llm\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  HuggingFace failed: {e}\")\n",
    "    \n",
    "    # Try OpenAI last\n",
    "    if provider == \"openai\" or provider == \"auto\":\n",
    "        try:\n",
    "            from langchain_openai import ChatOpenAI\n",
    "            import os\n",
    "            \n",
    "            openai_token = os.getenv('OPENAI_API_KEY')\n",
    "            if openai_token and openai_token != \"your_openai_api_key_here\":\n",
    "                openai_model = model_name or \"gpt-4\"\n",
    "                \n",
    "                openai_llm = ChatOpenAI(\n",
    "                    model=openai_model,\n",
    "                    temperature=0.1\n",
    "                )\n",
    "                \n",
    "                print(f\"‚úÖ Created OpenAI LLM: {openai_model}\")\n",
    "                return openai_llm\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  OpenAI failed: {e}\")\n",
    "    \n",
    "    print(\"‚ùå All LLM providers failed\")\n",
    "    return None\n",
    "\n",
    "# SQL Agent Functions\n",
    "def write_sql_query(llm):\n",
    "    \"\"\"Generate SQL query from natural language\"\"\"\n",
    "    template = \"\"\"Based on the table schema below, write a SQL query that would answer the user's question:\n",
    "    {schema}\n",
    "\n",
    "    Question: {question}\n",
    "    SQL Query:\"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Given an input question, convert it to a SQL query. No pre-amble. Return only the SQL query.\"),\n",
    "        (\"human\", template),\n",
    "    ])\n",
    "\n",
    "    return (\n",
    "        RunnablePassthrough.assign(schema=get_schema)\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "def answer_user_query(query, llm):\n",
    "    \"\"\"Complete SQL agent - generates SQL, executes it, and provides natural language response\"\"\"\n",
    "    template = \"\"\"Based on the table schema below, question, sql query, and sql response, write a natural language response:\n",
    "    {schema}\n",
    "\n",
    "    Question: {question}\n",
    "    SQL Query: {query}\n",
    "    SQL Response: {response}\"\"\"\n",
    "\n",
    "    prompt_response = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Given an input question and SQL response, convert it to a natural language answer. No pre-amble.\"),\n",
    "        (\"human\", template),\n",
    "    ])\n",
    "\n",
    "    full_chain = (\n",
    "        RunnablePassthrough.assign(query=write_sql_query(llm))\n",
    "        | RunnablePassthrough.assign(\n",
    "            schema=get_schema,\n",
    "            response=lambda x: run_query(x[\"query\"]),\n",
    "        )\n",
    "        | prompt_response\n",
    "        | llm\n",
    "    )\n",
    "\n",
    "    return full_chain.invoke({\"question\": query})\n",
    "\n",
    "print(\"‚úÖ Unified LLM functions created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a822b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_user_query(query, llm):\n",
    "    template = \"\"\"Based on the table schema below, question, sql query, and sql response, write a natural language response:\n",
    "    {schema}\n",
    "\n",
    "    Question: {question}\n",
    "    SQL Query: {query}\n",
    "    SQL Response: {response}\"\"\"\n",
    "\n",
    "    prompt_response = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"Given an input question and SQL response, convert it to a natural language answer. No pre-amble.\",\n",
    "            ),\n",
    "            (\"human\", template),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    full_chain = (\n",
    "        RunnablePassthrough.assign(query=write_sql_query(llm))\n",
    "        | RunnablePassthrough.assign(\n",
    "            schema=get_schema,\n",
    "            response=lambda x: run_query(x[\"query\"]),\n",
    "        )\n",
    "        | prompt_response\n",
    "        | llm\n",
    "    )\n",
    "\n",
    "    return full_chain.invoke({\"question\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4b6f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üõ†Ô∏è HELPER FUNCTIONS\n",
    "\n",
    "def simple_db_query(sql_query):\n",
    "    \"\"\"Run a direct SQL query on the database\"\"\"\n",
    "    try:\n",
    "        return db.run(sql_query)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def ask_database(question):\n",
    "    \"\"\"Simple function to ask questions to your database\"\"\"\n",
    "    if 'working_llm' in globals():\n",
    "        try:\n",
    "            response = answer_user_query(question, llm=working_llm)\n",
    "            return response.content\n",
    "        except Exception as e:\n",
    "            return f\"Error: {e}\"\n",
    "    else:\n",
    "        return \"Error: No LLM available. Run the setup cells first.\"\n",
    "\n",
    "def show_database_info():\n",
    "    \"\"\"Display basic information about the database\"\"\"\n",
    "    try:\n",
    "        artists = simple_db_query(\"SELECT COUNT(*) FROM Artist\")[0][0]\n",
    "        albums = simple_db_query(\"SELECT COUNT(*) FROM Album\")[0][0] \n",
    "        tracks = simple_db_query(\"SELECT COUNT(*) FROM Track\")[0][0]\n",
    "        genres = simple_db_query(\"SELECT COUNT(*) FROM Genre\")[0][0]\n",
    "        \n",
    "        print(\"üìä Database Information:\")\n",
    "        print(f\"   Artists: {artists}\")\n",
    "        print(f\"   Albums: {albums}\")\n",
    "        print(f\"   Tracks: {tracks}\")\n",
    "        print(f\"   Genres: {genres}\")\n",
    "        \n",
    "        print(f\"\\nüéµ Sample Artists:\")\n",
    "        sample_artists = simple_db_query(\"SELECT Name FROM Artist LIMIT 5\")\n",
    "        for i, (name,) in enumerate(sample_artists, 1):\n",
    "            print(f\"   {i}. {name}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting database info: {e}\")\n",
    "\n",
    "print(\"‚úÖ Helper functions created:\")\n",
    "print(\"   ‚Ä¢ simple_db_query('SQL HERE')\")\n",
    "print(\"   ‚Ä¢ ask_database('Your question')\")\n",
    "print(\"   ‚Ä¢ show_database_info()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a26a16",
   "metadata": {},
   "source": [
    "# üéâ **STATUS REPORT: All Code Issues Fixed!**\n",
    "\n",
    "## ‚úÖ **What's Working:**\n",
    "1. **Database Connection**: ‚úÖ Chinook.db found and connected (1,003,520 bytes)\n",
    "2. **Code Structure**: ‚úÖ All parameter errors fixed\n",
    "3. **Prerequisites**: ‚úÖ All setup cells working\n",
    "4. **Error Handling**: ‚úÖ Proper fallbacks added\n",
    "\n",
    "## ‚ö†Ô∏è **What You Need to Add:**\n",
    "**API Keys in your `.env` file** - Choose one or both:\n",
    "\n",
    "### **Option 1: OpenAI (Recommended - Most Reliable)**\n",
    "```env\n",
    "OPENAI_API_KEY=sk-your-actual-openai-key-here\n",
    "```\n",
    "- Get from: https://platform.openai.com/api-keys\n",
    "- More reliable, better SQL generation\n",
    "\n",
    "### **Option 2: HuggingFace (Free Alternative)**\n",
    "```env\n",
    "HUGGINGFACEHUB_API_TOKEN=hf_your-actual-huggingface-token-here\n",
    "```\n",
    "- Get from: https://huggingface.co/settings/tokens\n",
    "- Free but less reliable\n",
    "\n",
    "## üöÄ **Next Steps:**\n",
    "1. **Add API key(s)** to your `.env` file\n",
    "2. **Restart notebook kernel** to load new environment variables\n",
    "3. **Run the \"SIMPLE TEST\" cell** again\n",
    "4. **Enjoy your working SQL Agent!** ü§ñ\n",
    "\n",
    "## üõ†Ô∏è **Fixed Issues:**\n",
    "- ‚ùå Parameter validation errors ‚Üí ‚úÖ Fixed `temperature` parameter placement\n",
    "- ‚ùå Missing dependencies ‚Üí ‚úÖ Added proper imports and setup\n",
    "- ‚ùå Database path issues ‚Üí ‚úÖ Fixed absolute path handling  \n",
    "- ‚ùå No error handling ‚Üí ‚úÖ Added comprehensive fallbacks\n",
    "- ‚ùå Missing prerequisites ‚Üí ‚úÖ Created setup cell\n",
    "\n",
    "**Your SQL Agent is 95% ready - just needs API keys!** üéØ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e2e323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ SQL AGENT - Simple Demo\n",
    "\n",
    "print(\"\udd16 Creating SQL Agent...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create LLM (tries Ollama first, then HuggingFace, then OpenAI)\n",
    "working_llm = create_llm(\"auto\")\n",
    "\n",
    "if working_llm is None:\n",
    "    print(\"‚ùå No LLM available. Make sure you have:\")\n",
    "    print(\"   ‚Ä¢ Ollama running with a model (ollama pull llama3.2)\")\n",
    "    print(\"   ‚Ä¢ OR HuggingFace token in .env file\")\n",
    "    print(\"   ‚Ä¢ OR OpenAI API key in .env file\")\n",
    "else:\n",
    "    print(f\"‚úÖ LLM ready: {type(working_llm).__name__}\")\n",
    "    \n",
    "    # Test the SQL agent with sample questions\n",
    "    test_questions = [\n",
    "        \"How many artists are in the database?\",\n",
    "        \"Show me 3 artists whose names start with 'A'\",\n",
    "        \"How many albums are there?\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nüß™ Testing SQL Agent...\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    for i, question in enumerate(test_questions, 1):\n",
    "        print(f\"\\n{i}. Question: {question}\")\n",
    "        try:\n",
    "            response = answer_user_query(question, llm=working_llm)\n",
    "            print(f\"   Answer: {response.content}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   Error: {e}\")\n",
    "            # Fallback to direct database query\n",
    "            if \"artists\" in question.lower():\n",
    "                fallback = simple_db_query(\"SELECT COUNT(*) FROM Artist\")\n",
    "                print(f\"   Fallback: {fallback[0][0]} artists found\")\n",
    "            elif \"albums\" in question.lower():\n",
    "                fallback = simple_db_query(\"SELECT COUNT(*) FROM Album\") \n",
    "                print(f\"   Fallback: {fallback[0][0]} albums found\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 50)\n",
    "    print(\"üéâ SQL Agent Ready! Use:\")\n",
    "    print(\"answer_user_query('Your question here', llm=working_llm)\")\n",
    "    print(\"simple_db_query('SELECT * FROM Artist LIMIT 5')\")\n",
    "    \n",
    "    # Save for easy access\n",
    "    globals()['sql_agent_llm'] = working_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75435b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \udca1 USAGE EXAMPLES\n",
    "\n",
    "print(\"üí° How to use your SQL Agent:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Example 1: Ask database questions\n",
    "print(\"\\n1Ô∏è‚É£ Ask natural language questions:\")\n",
    "print(\"answer_user_query('How many tracks are there?', llm=working_llm)\")\n",
    "print(\"answer_user_query('What genres are available?', llm=working_llm)\")\n",
    "print(\"answer_user_query('Which artist has the most albums?', llm=working_llm)\")\n",
    "\n",
    "# Example 2: Direct SQL queries  \n",
    "print(\"\\n2Ô∏è‚É£ Run direct SQL queries:\")\n",
    "print(\"simple_db_query('SELECT * FROM Artist LIMIT 5')\")\n",
    "print(\"simple_db_query('SELECT COUNT(*) FROM Track')\")\n",
    "print(\"simple_db_query('SELECT DISTINCT Name FROM Genre')\")\n",
    "\n",
    "# Example 3: Direct LLM chat\n",
    "print(\"\\n3Ô∏è‚É£ Chat directly with LLM:\")\n",
    "print(\"working_llm.invoke([HumanMessage(content='Explain what SQL is')])\")\n",
    "\n",
    "print(\"\\nüéØ Try one now:\")\n",
    "if 'working_llm' in globals():\n",
    "    try:\n",
    "        # Quick test\n",
    "        result = answer_user_query(\"How many artists are there?\", llm=working_llm)\n",
    "        print(f\"‚úÖ Test result: {result.content}\")\n",
    "    except:\n",
    "        result = simple_db_query(\"SELECT COUNT(*) FROM Artist\")\n",
    "        print(f\"‚úÖ Database has {result[0][0]} artists\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Run the previous cell first to create working_llm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63fcfec",
   "metadata": {},
   "source": [
    "# üéâ Clean SQL Agent Notebook - Ready to Use!\n",
    "\n",
    "## üìã **What This Notebook Contains:**\n",
    "\n",
    "### **Core Cells (Run in Order):**\n",
    "1. **Install Dependencies** - Installs all required packages\n",
    "2. **Import Libraries** - Imports LangChain and database libraries  \n",
    "3. **Setup Prerequisites** - Database connection and environment variables\n",
    "4. **Unified LLM Creation** - One function for Ollama, HuggingFace & OpenAI\n",
    "5. **SQL Agent Functions** - Complete SQL query generation and execution\n",
    "6. **Helper Functions** - Simple utility functions\n",
    "7. **SQL Agent Demo** - Test and create your working LLM\n",
    "8. **Usage Examples** - How to use the agent\n",
    "\n",
    "### **Available Functions:**\n",
    "- `create_llm(\"auto\")` - Creates best available LLM (Ollama ‚Üí HuggingFace ‚Üí OpenAI)\n",
    "- `answer_user_query(\"question\", llm=working_llm)` - Complete SQL agent\n",
    "- `simple_db_query(\"SQL query\")` - Direct database queries\n",
    "- `ask_database(\"question\")` - Simple question interface\n",
    "- `show_database_info()` - Display database statistics\n",
    "\n",
    "### **LLM Provider Priority:**\n",
    "1. **Ollama** (Local, fast, private) - `create_llm(\"ollama\")`\n",
    "2. **HuggingFace** (Free API) - `create_llm(\"huggingface\")`  \n",
    "3. **OpenAI** (Paid, high quality) - `create_llm(\"openai\")`\n",
    "\n",
    "### **Ready to Use!**\n",
    "Your notebook is now clean and streamlined with only the essential methods for each LLM provider."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
